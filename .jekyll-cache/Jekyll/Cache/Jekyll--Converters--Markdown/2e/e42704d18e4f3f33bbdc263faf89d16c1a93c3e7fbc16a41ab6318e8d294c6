I"F<p>Hey there! Let‚Äôs recap what‚Äôs been happening with this Iowa housing dataset:</p>

<ul>
  <li><a href="/blog/2020/08/05/iowa-housing-exploration">Here</a> I explored the dataset using visualizations and python‚Äôs <code>seaborn</code> library</li>
  <li><a href="/blog/2020/11/11/filling-NA-values">Here</a> I talked about filling a categorical NA using a specific logical flow so the dataset can keep more features, using python‚Äôs <code>pandas</code> library</li>
  <li><a href="/blog/2021/03/17/iowa-data-cleaning">Here</a> I cleaned up any remaining NAs in the dataset by utilizing <code>pandas</code>‚Äôs robust <code>fillNA</code> function</li>
</ul>

<p>Now that we have a freshly cleaned dataset to work with, I want to select the features from the dataset that appear to have the most impact on the output <code>SalePrice</code>. This can be done for a number of reasons, usually to reduce computation time / complexity, thereby improving a model‚Äôs performance by removing redunant or unneccessary features. In this case, I also want to use feature selection to help me evaluate which features could be combined or transformed to create a feature with more prediction power.</p>

<p>Another item of note is our previously established baseline: via the <a href="">Kaggle</a> competition, by using the features <code>LotArea</code>, <code>YearBuilt</code>, <code>1stFlrSF</code>, <code>2ndFlrSF</code>, <code>FullBath</code>, <code>BedroomAbvGr</code>, <code>TotRmsAbvGrd</code>, a simple regression model predicts <code>SalePrice</code> with a Mean Absolute Error (MAE) of $21, 857 and 85.86% accuracy.</p>

<p align="center">
<img src="/assets/images/2021-06-14/wolfofwallstreet-rookienumbers.gif" alt="Matthew McConaughey in Wolf of Wall Street speaking the line 'Those are rookie numbers'" />
<br />
<sup><sub>image via <a href="https://tenor.com/view/rookie-numbers-wolf-of-wallstreet-mark-hanna-matthew-mcconaughey-gif-17120857">tenor</a></sub></sup>
</p>

<p>Let‚Äôs get our packages and dataset loaded and ready for some crunching:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># import the usual suspects
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># read in the datasets
</span><span class="n">train_data_path</span> <span class="o">=</span> <span class="s">'cleaned-data/cleaned_ihd_train.csv'</span>
<span class="n">test_data_path</span> <span class="o">=</span> <span class="s">'cleaned-data/cleaned_ihd_test.csv'</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">train_data_path</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">test_data_path</span><span class="p">)</span>

<span class="c1"># drop unneccessary columns from the datasets
</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Unnamed: 0'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Id'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Unnamed: 0'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Id'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># specify X as features and Y as target variable
</span><span class="n">X</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'SalePrice'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span>

<span class="c1"># make list of categorical and numerical features
</span><span class="n">isCat</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="s">'object'</span>
<span class="n">cat_feat</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">dtypes</span><span class="p">[</span><span class="n">isCat</span><span class="p">].</span><span class="n">index</span><span class="p">)</span>
<span class="n">num_feat</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">dtypes</span><span class="p">[</span><span class="o">~</span><span class="n">isCat</span><span class="p">].</span><span class="n">index</span><span class="p">)</span>

<span class="c1"># save feature names in list
</span><span class="n">feature_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># create categorical encoder
# (should be no unknowns to handle in dataset)
</span><span class="n">categorical_encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

<span class="c1"># create numerical handler
# (handle unknowns by imputing the mean,
# and scales numerical features)
</span><span class="n">numerical_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'imputer'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)),</span>
     <span class="p">(</span><span class="s">'scaler'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())])</span>

<span class="c1"># create preprocessing step
</span><span class="n">preprocessing</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="p">[(</span><span class="s">'cat'</span><span class="p">,</span> <span class="n">categorical_encoder</span><span class="p">,</span> <span class="n">cat_feat</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'num'</span><span class="p">,</span> <span class="n">numerical_pipe</span><span class="p">,</span> <span class="n">num_feat</span><span class="p">)])</span></code></pre></figure>

<p>Our data is loaded, scaled, encoded, and ready to be passed to a model. First up, performing feature selection via a random forest regressor.</p>

<hr />

<h3 id="method-1-random-forest-regressor">Method 1: Random Forest Regressor</h3>

<p>This is a fun method (I say fun because I like trees as a data structure, YMMV) that uses a random forest <em>regressor</em>* to perform feature selection. Below are some clear and concise posts about random forest algorithms and the underlying decision tree structure that explain the logic behind the algorithm and provide coding examples to get started:</p>

<p><sup>* Yes, I did spend the better part of half an hour debugging a classifer. My big clue that something was up was the 97% accuracy on the training set, and 1.4% accuracy on the cross val test. My first thought was severe overfitting. Live and learn!</sup></p>
<ul>
  <li><a href="https://towardsdatascience.com/random-forest-in-python-24d0893d51c0">Will Koehrsen | Random Forest in Python</a></li>
  <li><a href="https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e">Eryk Lewinson | Explaining Feature Importance by example of a Random Forest</a></li>
  <li><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html">scikit-learn forest importances documentation</a></li>
</ul>

<p>An excellent point made by <a href="">Jason Brownlee</a>: ‚Äú‚Ä¶ some machine learning algorithms (‚Ä¶) perform feature selection automatically as part of learning the model. We might refer to these techniques as <em>intrinsic</em> feature selection methods.‚Äù This is the reason we are using the random forest regressor as our first method for feature selection - the algorithm automatically performs feature selection when training, so we can check out each feature‚Äôs importance to the algorithm after the training step.</p>

<ul>
  <li>IMPORTANT: there is something important happening once categorical variables are one hot encoded; I found the most enlightenment reading <a href="https://stats.stackexchange.com/questions/314567/feature-importance-with-dummy-variables">this</a> stackexchange post, and following up with the scikit-learn documentation. In a nutshell: recall we have a categorical feature <code>Neighborhood</code>; there are 25 neighborhoods featured in our dataset. If we one hot encode our neighborhood variable, then try to test for feature importance, we‚Äôre going to end up with the individual importances of each neighborhood to predict <code>SalePrice</code>, and not the importance of the feature <code>Neighborhood</code> to predict <code>SalePrice</code>. It‚Äôs subtle to think about, but it can drastically increase the time complexity of full permutation importance, as features are shuffled n times and the model refitted to estimate the importance of each feature. After one hot encoding this dataset, the features grow from 80 to over 300, which adds up fast!</li>
</ul>

<p>Using <code>scikit-learn</code>‚Äôs permutation importance, we see the following:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># create numerical handler
# (handle unknowns by imputing the mean,
# and scales numerical features)
</span><span class="n">numerical_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'imputer'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)),</span>
     <span class="p">(</span><span class="s">'scaler'</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())])</span>
<span class="c1"># create preprocessing transform step
</span><span class="n">preprocessing</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="p">[(</span><span class="s">'cat'</span><span class="p">,</span> <span class="n">categorical_encoder</span><span class="p">,</span> <span class="n">cat_feat</span><span class="p">),</span>
     <span class="p">(</span><span class="s">'num'</span><span class="p">,</span> <span class="n">numerical_pipe</span><span class="p">,</span> <span class="n">num_feat</span><span class="p">)])</span>

<span class="c1"># create actual pipeline
</span><span class="n">rf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'preprocess'</span><span class="p">,</span> <span class="n">preprocessing</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'regressor'</span><span class="p">,</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
        <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
        <span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
        <span class="n">max_features</span> <span class="o">=</span> <span class="s">'auto'</span><span class="p">,</span>
        <span class="n">oob_score</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))])</span></code></pre></figure>

<p align="center">
<img src="/assets/images/2021-06-14/rfpermimpbarplot.svg" alt="Bar plot of 10 highest permutation importance values" width="100%" />
</p>

<p>Our take away here: the feature with the highest importance is deemed <code>OverallQual</code>. After <code>OverallQual</code>, most of the features give us some measure of the home's square footage, so we will look at combining and transforming the home's many area measurements in this post's feature engineering section.</p>

<hr />

<h3 id="method-2-selectkbest">Method 2: SelectKBest</h3>

<ul>
  <li><a href="https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/">Jason Brownlee | How to Choose a Feature Selection Method For Machine Learning</a></li>
</ul>

<p>In this section, instead of going through all the trouble of making and training a random forest regressor, we're going to use some statistical tests to calculate correlation between each feature and our target <code>SalePrice</code>. As referenced above, correlation between numerical and categorical variables require different statistical tests. We can create a pipeline that will do just that for us:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># create numerical handler
# (performs statistical test of our choice
# to test for correlation)</span></code></pre></figure>

<hr />

<h3 id="method-3-lasso-tools">Method 3: Lasso Tools</h3>

<h4 id="lasso-regression-vs-ridge-regression-vs-elastic-net-regression">Lasso Regression vs. Ridge Regression vs. Elastic Net Regression</h4>

<p>My understanding:</p>

<p>Ridge Regression - used for regularization (stops model from over fitting by imposing a penalty on the size of the model‚Äôs parameters; onto your model prediction equation, you add a ‚Äúregularization‚Äù term - lambda (any positive value) multiplied by the sum of the squares of your parameters, thus stopping them from getting out of hand)</p>

<p>Problem - if you have a lot of useless features, they will still show up in your model's prediction equation and waste a lot of resources doing matrix multiplication.</p>

<p>Lasso Regression - also used for regularization (gets rid of the problem with Ridge by making the penalty term lambda multiplied by the sum of the absolute values of your parameters, thus allowing some parameters to be reduced to 0 (and telling you which ones the model doesn‚Äôt rely on for an accurate prediction))</p>

<p>Elastic Net Regression - combines Lasso and Ridge into one equation with 2 lambdas, one for each penalty term; good at dealing with situations where there are correlations between 2 parameters</p>

<hr />

<p>As a departing note, one thing that has become clear as I add more tools to my data analysis belt: <em>domain knowledge</em> really makes the difference between a surface level analysis and a comprehensive deep dive. You can know all the techniques and technologies, but if you do not know the context of your data, your reports will fall flat. <a href="https://nycdatascience.com/blog/r/ames-iowa-real-estate-analysis/">Other</a> analysts' approach to this dataset, when motivated by the right questions (not just predicting <code>SalePrice</code>, but tracking construction trends, finding profitable additions to homes) reveals the depth of information contained in the same dataset. It's a great (and slightly terrifying!) thing to realize, that there is so much more to learn based on the industry one supports as an analyst.</p>
:ET