I"
8<p>In my last post about the Iowa housing data set, I spent some time working with a specific logical flow for filling categorical NA values - you can find that post <a href="/blog/2020/11/11/filling-NA-values">here</a>.
In the post previous to that, I visualized some features, using a lot of box / violin plots, count plots, and some linear regression scatter plots comparing the distribution of SalePrices when sorted by specific features. If you would like a refresher, check that post out <a href="/blog/2020/08/05/iowa-housing-exploration">here</a>.</p>

<p>For today’s post, I’m going to walk through my process of cleaning the data. Thanks to Kaggle users <a href="https://www.kaggle.com/datafan07">Ertuğrul Demir</a> and <a href="https://www.kaggle.com/goldens">Golden</a> for posting their in-depth notebooks. Both contain a wealth of information and provide a great starting base for other novice data analysts.</p>

<hr />

<h3 id="examining-the-missing-data">examining the missing data</h3>

<p>By now, we should be quite familiar with the data set (unless you haven’t been working with the data set daily like I have…), but let’s go ahead and summarize the numerical data once more. Below, you’ll see some <b>count</b> cells are highlighted in orange - these are features with missing values.</p>

<p><code> &gt;&gt;&gt; train_data.describe()</code></p>

<iframe src="/assets/html-tables/train_data_describe.html" width="100%" height="225px"></iframe>

<p><code> &gt;&gt;&gt; test_data.describe()</code></p>

<iframe src="/assets/html-tables/test_data_describe.html" width="100%" height="225px"></iframe>

<p>The first thing I’d like to do is figure out what’s going on with any null / NA values in both the train and test set, so I can keep as much information in my data as possible. I merged the 2 data sets, and created a summary table with an overview of unique values for categorical features, the min/max/mean/median for numerical features, and percentage of samples that have an NA value:</p>

<p><code> &gt;&gt;&gt; python missingValues.py </code></p>

<iframe src="/assets/html-tables/missing-values.html" width="100%" height="280px"></iframe>

<hr />

<h3 id="filling-the-missing-data">filling the missing data</h3>

<p>In this section, we’re going to fill the missing NA values using logical structures inside the <code>fillna</code> method.</p>

<h4 id="categorical-features-with-na--nan-which-mean-none">categorical features with “NA / nan” which mean “none”</h4>
<h5 id="poolqc-miscfeature-alley-fence-fireplacequ-garagefinish-garagequal-garagecond-garagetype-bsmtcond-bsmtexposure-bsmtqual-bsmtfintype1-bsmtfintype2-masvnrtype">PoolQC, MiscFeature, Alley, Fence, FireplaceQu, GarageFinish, GarageQual, GarageCond, GarageType, BsmtCond, BsmtExposure, BsmtQual, BsmtFinType1, BsmtFinType2, MasVnrType</h5>

<p>For many of the categorical features, <code>NA</code> is used if the sample doesn’t contain the given feature. For these samples, we will change the sample feature to <code>None</code> instead.</p>

<p><code>none_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType', 'BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']</code></p>

<pre><code>for col in none_cols:
    train_data[col] = train_data[col].fillna("None")
    test_data[col] = test_data[col].fillna("None")
</code></pre>

<h4 id="numerical-features-with-na--nan-which-mean-0">Numerical features with “NA / nan” which mean 0</h4>
<h5 id="garageyrblt-masvnrarea-bsmtfullbath-bsmthalfbath-bsmtfinsf1-bsmtfinsf2-bsmtunfsf-totalbsmtsf-garagecars-garagearea">GarageYrBlt, MasVnrArea, BsmtFullBath, BsmtHalfBath, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, GarageCars, GarageArea</h5>

<p>These numerical features just need 0 instead of <code>NA</code>, since the feature doesn’t exist.</p>

<p><code>zero_cols = ['GarageYrBlt', 'MasVnrArea', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']</code></p>

<pre><code>for col in zero_cols:
	train_data[col] = train_data[col].fillna(0, inplace=True)
	test_data[col] = test_data[col].fillna(0, inplace=True)
</code></pre>

<h4 id="categorical-features-with-na--nan-which-need-examination">Categorical features with “NA / nan” which need examination</h4>
<h5 id="mszoning-functional-exterior1st-exterior2nd-electrical-kitchenqual-saletype-utilities">MSZoning, Functional, Exterior1st, Exterior2nd, Electrical, KitchenQual, SaleType, Utilities</h5>

<p>These are categorical features which are missing from 1-4 samples.</p>

<h5 id="mszoning">MSZoning</h5>

<p><em>This process is the subject of another post, which can be found <a href="/_posts/2020-11-11-filling-NA-values">here</a>!</em></p>

<p>Here’s the short snippet of code, included in my <code>iowaprerun.py</code> script:</p>

<pre>for ds in (test_data, train_data, merge_data):
    ds['MSZoning'] = ds['MSZoning'].fillna(ds.groupby('Neighborhood')['MSZoning'].transform(lambda x: x.mode().iloc[0]))</pre>

<h5 id="functional">Functional</h5>

<p>It appears <code>Functional</code> measures how many safety deductions the house has / how much overall damage there is, which we will try to predict using 4 features: <code>OverallCond, BsmtCond, ExterCond, GarageCond</code>. These features should give us an idea of any damage to the home, and how <code>Functional</code> is affected.</p>

<p><code> &gt;&gt;&gt; condFeatures = ['Id', 'Functional', 'OverallCond', 'BsmtCond', 'ExterCond', 'GarageCond'] </code></p>

<p>Only 2 samples from the test set are missing this feature:</p>

<p><code> &gt;&gt;&gt; mask4 = merge_data['Functional'].isna() </code></p>

<p><code> &gt;&gt;&gt; merge_data[mask4][condFeatures]</code></p>

<pre><code>        Id Functional  OverallCond BsmtCond ExterCond GarageCond
2216  2217        NaN            5     None        Po         Po
2473  2474        NaN            1       Fa        Fa         Fa
</code></pre>

<p>After looking at this table, I thought it would be easier to parse this data if the features were encoded as an integer instead of a categorical variable (which is only really a problem if we start to question the “scale” of damage and if it’s truly linear…). Still, I decided to go ahead and recode all ‘condFeatures’ features using a integer scale (I’ll do this to all categorical variables later):</p>
<pre><code>cond_map = {       |     func_map = {
    'None': 0,     |        'Sal'  : 0,
    'Po'  : 1,     |        'Sev'  : 1,
    'Fa'  : 2,     |        'Maj2' : 2,
    'TA'  : 3,     |        'Maj1' : 3,
    'Gd'  : 4,     |        'Mod'  : 4,
    'Ex'  : 5      |        'Min2' : 5,
}                  |        'Min1' : 6,
                   |        'Typ'  : 7
                   |     }

&gt;&gt;&gt; merge_data['BsmtCond'] = merge_data['BsmtCond'].map(cond_map).astype('int')
&gt;&gt;&gt; merge_data['ExterCond'] = merge_data['ExterCond'].map(cond_map).astype('int')
&gt;&gt;&gt; merge_data['GarageCond'] = merge_data['GarageCond'].map(cond_map).astype('int')</code></pre>

<p>Then, I put all the examples with <code>Functional</code> values into a dataframe to do some correlation analysis, and map the functionality to the <code>func_map</code> above:</p>

<pre><code>&gt;&gt;&gt; mask5 = merge_data['Functional'].isna()
&gt;&gt;&gt; condfeatdf = merge_data[~mask5][condFeatures].copy()
</code></pre>

<pre><code>
&gt;&gt;&gt; condfeatdf['Functional'] = condfeatdf['Functional'].map(func_map).astype('int')
&gt;&gt;&gt; condfeatdf.corr()[1:][['Functional', 'OverallCond', 'BsmtCond', 'ExterCond', 'GarageCond']]
</code></pre>

<pre><code>             Functional  OverallCond  BsmtCond  ExterCond  GarageCond
Functional                  0.118222  0.190537   0.074505    0.090145
OverallCond                           0.090408   0.403052    0.045268
BsmtCond                                         0.096040    0.140137
ExterCond                                                    0.093949
GarageCond                                                           
</code></pre>

<p>Going to leave this for now, since I’m a little tired of looking at it, come back to this one</p>

<h5 id="exterior-1st--exterior-2nd">Exterior 1st / Exterior 2nd</h5>

<p>There’s only 1 sample that is missing both Ext1 and Ext2 features:</p>
<pre>&gt;&gt;&gt; mask6 = merge_data['Exterior1st'].isna()
&gt;&gt;&gt; mask7 = merge_data['Exterior2nd'].isna()
&gt;&gt;&gt; merge_data[mask6 &amp; mask7]['ExterQual', 'ExterCond', 'Neighborhood', 'MSSubClass']

     ExterQual  ExterCond Neighborhood
2151        TA          3      Edwards</pre>

<p>We’ll go ahead and assign the value according to the most common in the <code>Neighborhood</code>, using the same logical structure as we used for filling <code>MSZoning</code> NAs.</p>

<pre>&gt;&gt;&gt; for col in ('Exterior1st', 'Exterior2nd'):
&gt;&gt;&gt;    for ds in (test_data, train_data, merge_data):
&gt;&gt;&gt;        ds[col] = ds[col].fillna(ds.groupby('Neighborhood')[col].transform(lambda x: x.mode().iloc[0]))</pre>

<h5 id="electrical--kitchenqual--saletype--utilities">Electrical / KitchenQual / SaleType / Utilities</h5>

<p>For these variables, there’s only 1-2 samples missing each feature, so we’re just going to fill it with the mode of the feature from the dataset.</p>

<pre>&gt;&gt;&gt; for col in ('Electrical', 'KitchenQual', 'SaleType', 'Utilities'):
&gt;&gt;&gt;    for ds in (test_data, train_data, merge_data):
&gt;&gt;&gt;        ds[col] = ds[col].fillna(ds[col].mode()[0]</pre>

<h4 id="numerical-features-with-na--nan-which-need-examination">Numerical features with “NA / nan” which need examination</h4>
<h5 id="lotfrontage-garagearea-garagecars">LotFrontage, GarageArea/ GarageCars</h5>

<p>There’s a little more happening here than what appears at surface level, so let’s take a deeper dive into these features:</p>

<h5 id="lotfrontage">LotFrontage</h5>

<p>This is one of the features which a significant number of samples (486 / 2919) are missing. Lot frontage is defined as “linear feet of street connected to property.” Certainly we can draw the conclusion that <code>LotArea</code> might be correlated to <code>LotFrontage</code> since one is used to calculate the other, but there’s a few other features that can help us fill these in with greater accuracy.</p>

<p>First, let’s see the correlation between <code>LotArea</code> and <code>LotFrontage</code>.</p>

<pre>&gt;&gt;&gt; merge_data[['LotArea', 'LotFrontage']].corr()
              LotArea  LotFrontage
LotArea      1.000000     0.489896
LotFrontage  0.489896     1.000000</pre>

<p>0.48 isn’t the best we’ve seen - we can see something interesting happening if we sort these by another variable, <code>LotConfig</code>, which gives us more information about the layout of the lot.</p>

<pre>LotConfig: Lot configuration

       Inside   Inside lot
       Corner   Corner lot
       CulDSac  Cul-de-sac
       FR2  Frontage on 2 sides of property
       FR3  Frontage on 3 sides of property</pre>

<p>Considering how Corner lots may have twice as much <code>LotFrontage</code> as Inside lots, and FR2s and FR3s should have comparitively more as well, this might help us calculate a more accurate prediction.</p>

<pre>&gt;&gt;&gt; h = sns.lmplot(x='LotArea', y='LotFrontage', data=merge_data[merge_data['LotArea'] &lt;= 80000], hue='LotConfig')
&gt;&gt;&gt; plt.show()
</pre>

<p><img src="/assets/images/yLotFrontagexLotAreabyLotConfig.png" /></p>

<p>Note - I limited the <code>LotArea &lt;= 80000</code> since there was one outlier which skewed the whole figure off to the right, at over 200000 square feet.</p>

<p>Another way we can understand how this yields a more accurate prediction is by checking the correlation between <code>LotArea</code> and <code>LotFrontage</code> again, but this time grouped by <code>LotConfig</code>.</p>

<pre>&gt;&gt;&gt; merge_data[merge_data['LotArea'] &lt;= 80000].groupby('LotConfig')[['LotArea','LotFrontage']].corr()
                        LotArea  LotFrontage
LotConfig                                   
Corner    LotArea      1.000000     0.787955
          LotFrontage  0.787955     1.000000
CulDSac   LotArea      1.000000     0.195327
          LotFrontage  0.195327     1.000000
FR2       LotArea      1.000000     0.827626
          LotFrontage  0.827626     1.000000
FR3       LotArea      1.000000     0.835891
          LotFrontage  0.835891     1.000000
Inside    LotArea      1.000000     0.630001
          LotFrontage  0.630001     1.000000</pre>

<p>Comparing this to our previous correlations, there’s a strict improvement for all <code>LotConfig</code> features except for <code>CulDSac</code> which could be due to the irregular shape of <code>CulDSac</code> lots, and their disproportionately small <code>LotFrontage</code> measure. I predict that we can use the linear relationship between <code>LotArea</code> and <code>LotFrontage</code> grouped by <code>LotConfig</code> to fill in our missing <code>LotFrontage</code> feature.</p>

<pre></pre>

<h5 id="garagearea--garagecars">GarageArea / GarageCars</h5>

<p>This one is interesting because upon first glance, I thought “ok, if <code>GarageArea</code> is <code>NA</code> it must be because there’s no garage.” However, after examining this sample’s <code>GarageType</code>, it’s not the expected <code>None</code> but <code>Detchd</code> (Detatched from home). Now we’ve got something interesting to work with!</p>

<pre><code>&gt;&gt;&gt; merge_data[merge_data['GarageCars'].isna()][['GarageType', 'GarageArea', 'GarageCars']]

     GarageType  GarageArea  GarageCars
2576     Detchd         NaN         NaN
</code></pre>

<p>There’s going to be two parts here - first, finding the mean of all homes with <code>Detchd</code> garages, then finding the average number of cars based on this mean.</p>

<pre><code>&gt;&gt;&gt; merge_data[merge_data['GarageType'] == 'Detchd']['GarageArea'].describe()
count     778.000000
mean      419.492288
std       174.726572
min       100.000000
25%       280.000000
50%       399.500000
75%       528.000000
max      1488.000000
</code></pre>

<p>We’ll use 419.5 as our mean here, and find how many cars on average fit into a garage of this size.</p>

:ET